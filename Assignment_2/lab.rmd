---
title: "Coursework assignment B - 2022-2023"
subtitle: "CS4125 Seminar Research Methodology for Data Science"
author: "Otte, Colin, Suhaib"
date: "16/04/2023"
output:
   pdf_document:
      fig_caption: true
      number_sections: true 
      keep_tex: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load data, include=FALSE}
d <- read.csv("data.csv")
table(d$model, d$TeD)
table(d$model, d$TrD1)
table(d$model, d$TrD2)
aggregate(score~model, data = d, mean)
library(dplyr)
```

```{r rq one, include=FALSE}
# t test between B1, B2, and B3 vs the rest
d_B <- subset(d, model=="B1" | model=="B2" | model=="B3")
d_M <- subset(d, model=="M1" | model=="M2" | model=="M3" | model=="MF" | model=="MN" | model=="MS")
t.test(d_B$score,d_M$score)

linear_model_on_models <- lm(score~model, d)
summary(linear_model_on_models)



anova(linear_model_on_models)
# plot the average score for each model
library(ggplot2)
ggplot(d, aes(x=model, y=score)) + geom_boxplot() + geom_jitter(width=0.2)

# Plot fitted values of the model with the residuals
plot(linear_model_on_models, which=1)

linear_model_model_trainD = lm(score~model * (TrD1 + TrD2 + TrD3 + TrD4 + TrD5 + TrD6 + TrD7 + TrD8), d)

library(emmeans)
emmeans(linear_model_model_trainD, ~model)

# Reorder the levels of the model variable
d$model <- factor(d$model, levels = c("M1", "M2", "M3", "MF", "MN", "MS"))

# Training set together with models
linear_model_TrD1 <- lm(score~model * (TrD1 + TrD2 + TrD3 + TrD4 + TrD5 + TrD6 + TrD7 + TrD8), d)
summary(linear_model_TrD1)

model_names = c("M1", "M2", "M3", "MF", "MN", "MS")
for (i in seq_along(model_names)) {
   current_model = model_names[i]
   d_current_model <- d[d$model == current_model, c("TeD", "TrD1", "TrD2", "TrD3", "TrD4", "TrD5", "TrD6", "TrD7", "TrD8", "score")]
   linear_model_current_model <- lm(score~TrD1 + TrD2 + TrD3 + TrD4 + TrD5 + TrD6 + TrD7 + TrD8, d_current_model)
   print(paste0("For: ", current_model))
   print(anova(linear_model_current_model))
}
```
### Data exploration
The experiment focused on two different sets of models: Baselines which were not trained with any training data and Transfer learning models which were trained on varying amounts of training data sets. The first research question asks “Does transfer learning improve over typical non-transfer learning?”. The boxplot below shows the distribution of the data specifically distinguishing the distributions for each model. Based this plot, there is a vast sampling imbalance between the baseline models and the transfer learning models. 

#### Visual: Box-plot
```{r boxplot, echo=FALSE, fig.width=4, fig.height=4}
ggplot(d, aes(x=model, y=score)) + geom_boxplot() + geom_jitter(width=0.1)

# Plot fitted values of the model with the residuals
plot(linear_model_on_models, which=1)
```

#### Transformed data for homogeneity
In the process of our analysis, we conducted a comprehensive investigation of our model's residuals and fitted values to evaluate the necessity of data transformation.

To gain insights into the residuals' structure, we plotted the residuals against the fitted values. There were no apparent trends, outliers, or structures, and the residuals seemed to be symmetrically distributed around a mean of zero across all levels of the fitted values.
The absence of noticeable patterns in the Residuals vs. Fitted plot indicates that the model fits the data adequately without a need for transformation. 


### RQ1: Does transfer learning improve over typical non-transfer learning?
#### T-test
```{r t-test, echo=FALSE}
d <- read.csv("data.csv")
d_B <- subset(d, model=="B1" | model=="B2" | model=="B3")
d_M <- subset(d, model=="M1" | model=="M2" | model=="M3" | model=="MF" | model=="MN" | model=="MS")
t.test(d_B$score,d_M$score)
```
An independent sample t-test was conducted to compare the performance scores between the baseline models (B1, B2, and B3) and the transfer sharing models (M1, M2, M3, MF, MN, and MS). Our results suggest that there is a statistically significant difference in the scores between these two groups t(20.307) = -2.2606, p = 0.03491. The mean performance score of the baseline models (M = 0.378) was found to be lower than the mean performance score of the transfer sharing models  (M = 0.496). The 95 percent confidence interval for the difference in means ranged from  -0.227 to -0.009 further indicating a significant difference between the two groups. This analysis does provide some indication that transfer sharing models perform significantly better than baseline models in our dataset. 

#### Average scores per model and Linear regression
To further investigate this difference, we also computed the mean performance scores for each model using the aggregate() function. Our analysis reveals variations in the average performance scores across different model types. Model MF displays the highest average performance score (0.531), while model B2 shows the lowest (0.328). This suggests that the choice of model may indeed have an impact on performance.

However, this analysis provides only average scores and does not account for the spread of the data within each model type. Further analysis, such as conducting an ANOVA test, would provide a more comprehensive understanding of the differences between the models. Thus we fitted a model with the score as the dependent variable and model as the independent variable. The overall model was significant (F(8, 2987) = 8.261, p < 0.001, Adjusted R-squared = 0.01903), suggesting there is a statistically significant difference in performance scores based on the model used. However, it's worth noting that the Adjusted R-squared value was relatively low, indicating that only a small percentage of the variability in performance scores could be accounted for by the model type. This implies other factors not accounted for in this analysis could significantly influence the performance score.

Estimate and standard error were calculated for each model, which are deviations from the intercept (B1 model) in terms of performance scores. However, none of these coefficients were found to be statistically significant (p > 0.05), suggesting no significant differences in performance scores between the B1 model and any other individual models when considered independently.

In conclusion, the analysis shows that while model type explains a portion of the variance in performance scores, the effect size is small, and considerable variance is left unexplained by features left out of the linear model. While the transfer sharing models do outperform the baseline models, performance within the transfer sharing group is not uniform, and other factors not considered in this analysis may significantly influence these results. This is what we will be exploring with the next research question.

### RQ2: How is model performance affected by the choice of training datasets?
The transfer learning models were first trained on 8 different training data sets. They all contain the same instances but differ in the particularities of the task (eg. classification, ranking, etc) and ground truth. The distribution of training data sets over the models was imbalanced. The m1 model had for instance used training data set 1 33% times more than training data set 2. 

```{r, echo=FALSE}
library(tidyverse)

# Reshape data
d_long <- d %>% pivot_longer(cols = starts_with("TrD"),
                             names_to = "TrD",
                             values_to = "value")

# Filter data for non-zero values (assuming 1 means the score belongs to that training dataset)
d_filtered <- d_long %>% filter(value == 1)

# Plot
ggplot(d_filtered, aes(x = TrD, y = score, color = TeD)) +
  geom_point(alpha = 0.6, position = position_jitter(w = 0.1, h = 0)) +
  theme_minimal() +
  labs(x = "Training Dataset", y = "Score",
       title = "Distribution of scores for each test dataset across training datasets",
       color = "Test Dataset") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Estimated marginal means (EMMs) were calculated for each model adjusted for the training sets used. These provide an estimate of the mean performance score for each model, adjusted for the training sets used by the models. The EMMs can not be calculated for the baseline models and MS since they don’t use any training data. The EMM differs slightly compared to the normal mean:

```{r, echo=FALSE}
model_list <- list()

model_list$MS <- d_M[d_M$model == "MS", c("TrD1", "TrD2", "TrD3", "TrD4", "TrD5", "TrD6", "TrD7", "TrD8", "TeD", "score", "Sum")]
model_list$MN <- d_M[d_M$model == "MN", c("TrD1", "TrD2", "TrD3", "TrD4", "TrD5", "TrD6", "TrD7", "TrD8", "TeD", "score", "Sum")]
model_list$M1 <- d_M[d_M$model == "M1", c("TrD1", "TrD2", "TrD3", "TrD4", "TrD5", "TrD6", "TrD7", "TrD8", "TeD", "score", "Sum")]
model_list$M2 <- d_M[d_M$model == "M2", c("TrD1", "TrD2", "TrD3", "TrD4", "TrD5", "TrD6", "TrD7", "TrD8", "TeD", "score", "Sum")]
model_list$M3 <- d_M[d_M$model == "M3", c("TrD1", "TrD2", "TrD3", "TrD4", "TrD5", "TrD6", "TrD7", "TrD8", "TeD", "score", "Sum")]
model_list$MF <- d_M[d_M$model == "MF", c("TrD1", "TrD2", "TrD3", "TrD4", "TrD5", "TrD6", "TrD7", "TrD8", "TeD", "score", "Sum")]

training_sets <- names(model_list$MS)[grepl("TrD", names(model_list$MS))]  # Extract TrD column names

combined_results <- data.frame()  # Initialize an empty data frame

for (train_set in training_sets) {
   for (model_name in names(model_list)) {
      model_data <- model_list[[model_name]]
      formula_str <- paste("score ~", train_set, " + TeD")
      score_sum_model <- lm(as.formula(formula_str), data = model_data)
      emm_TrD1 <- emmeans(score_sum_model, as.formula(paste("~", train_set)), data = model_data)
  
      # Get all pairwise comparisons, including non-significant ones
      pairwise_comp <- as.data.frame(pairs(emm_TrD1))
      # Add model name as a column to identify the results
      pairwise_comp$model_name <- model_name
      
      # Combine the pairwise comparison results
      combined_results <- bind_rows(combined_results, pairwise_comp)
   }
}

# Round p.value column to three decimal places
combined_results$p.value <- round(combined_results$p.value, 6)

# Filter out rows with absolute estimate smaller than 0.01 and p-value greater than 0.01
filtered_results <- combined_results %>%
  filter(abs(estimate) >= 0.01, p.value <= 0.01)


# Print the filtered results
if (nrow(filtered_results) > 0) {
  print(filtered_results)
} else {
  print("No pairwise comparisons found.")
}

```


```{r, echo=FALSE}
model_list <- list()

model_list$MS <- d_M[d_M$model == "MS", c("TrD1", "TrD2", "TrD3", "TrD4", "TrD5", "TrD6", "TrD7", "TrD8", "TeD", "score", "Sum")]
model_list$MN <- d_M[d_M$model == "MN", c("TrD1", "TrD2", "TrD3", "TrD4", "TrD5", "TrD6", "TrD7", "TrD8", "TeD", "score", "Sum")]
model_list$M1 <- d_M[d_M$model == "M1", c("TrD1", "TrD2", "TrD3", "TrD4", "TrD5", "TrD6", "TrD7", "TrD8", "TeD", "score", "Sum")]
model_list$M2 <- d_M[d_M$model == "M2", c("TrD1", "TrD2", "TrD3", "TrD4", "TrD5", "TrD6", "TrD7", "TrD8", "TeD", "score", "Sum")]
model_list$M3 <- d_M[d_M$model == "M3", c("TrD1", "TrD2", "TrD3", "TrD4", "TrD5", "TrD6", "TrD7", "TrD8", "TeD", "score", "Sum")]
model_list$MF <- d_M[d_M$model == "MF", c("TrD1", "TrD2", "TrD3", "TrD4", "TrD5", "TrD6", "TrD7", "TrD8", "TeD", "score", "Sum")]

training_sets <- names(model_list$MS)[grepl("TrD", names(model_list$MS))]  # Extract TrD column names

combined_results <- data.frame()  # Initialize an empty data frame
for (model_name in names(model_list)) {
   for (train_set in training_sets) {
      model_data <- model_list[[model_name]]
      formula_str <- paste("score ~", train_set, " + TeD")
      score_sum_model <- lm(as.formula(formula_str), data = model_data)
      emm_TrD1 <- emmeans(score_sum_model, as.formula(paste("~", train_set)), data = model_data)
  
      # Get all pairwise comparisons, including non-significant ones
      pairwise_comp <- as.data.frame(pairs(emm_TrD1))
      # Add model name as a column to identify the results
      pairwise_comp$model_name <- model_name
      
      # Combine the pairwise comparison results
      combined_results <- bind_rows(combined_results, pairwise_comp)
   }
}

# Round p.value column to three decimal places
combined_results$p.value <- round(combined_results$p.value, 6)

# Filter out rows with absolute estimate smaller than 0.01 and p-value greater than 0.01
filtered_results <- combined_results %>%
  filter(abs(estimate) >= 0.00000001, p.value <= 1)


# Print the filtered results
if (nrow(filtered_results) > 0) {
  print(filtered_results)
} else {
  print("No pairwise comparisons found.")
}

```
To find out if the differences are significant, a linear model was fitted on the scoring outcomes of the models, taking the training sets as independent variables. This was done on all the different transfer learning models. In most cases, there was no significant differences found but for the MS model the analysis found a significant effect  (F(1, 1) = 4.8530, p = 0.02829) for training set one and for the MS model the analysis found a significant effect (F(1, 1) = 8.7536, p = 0.003223) for training set one. 

Overall the effect of the training sets used is very small as the multiple R-squared value for the linear regression model was found to be 0.03657 (adjusted R-squared: 0.01943). This indicates that the combination of the independent variables, model type and TrD1-TrD8 variables, explains approximately 3.657% of the variance observed in the dependent variable, score.

Therefor it can be concluded that the training sets used for the models only has an insignificant impact on model performance. The relatively low R-squared value and the absence of significant effects in most models suggest that the choice of training sets does not considerably affect the scores. It's noteworthy, however, that specific models like MS have shown significant relationships with certain training datasets, indicating that for some models, their performance could be sensitive to the choice of training datasets.
